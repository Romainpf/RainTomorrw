# -*- coding: utf-8 -*-
"""fev22_bds_meteo_V4.1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ic-xPBgJkSdWxVzctIUi5hDsXWHan2Jn

# 0 - Exploration et découverte du jeu de données
"""

# Commented out IPython magic to ensure Python compatibility.
# Import des bibliothèques
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc, confusion_matrix
from imblearn.metrics import classification_report_imbalanced
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler, SMOTE
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from bokeh.io import output_file, show,output_notebook
from bokeh.plotting import figure, output_notebook, show
from bokeh.models import ColumnDataSource, GMapOptions, HoverTool
from bokeh.plotting import gmap
import xgboost as xgb
# %matplotlib inline
output_notebook()

# lire fichier weatherAUS.csv
df = pd.read_csv("weatherAUS.csv")

# Première informations sur les des données
print(df.info())
display(df.iloc[:,0:13].head())
display(df.iloc[:,13:].head())

# identification de la proportion de valeurs manquantes par variable
isna = pd.DataFrame(round(df.isnull().sum()/(df.shape[0])*100,2),columns=['% valeurs manquantes'])
isna
#on constate que les variables "Evaporation","Sunshine"et "Cloud3pm" on plus 40% de valeurs manquantes

#Création de nouvelles colonnes contenant les mois, jours, années et année + mois car ça nous sera util pour l'exploration
#Définition des fonctions à appliquer à la colonne 'Date'
def get_day(date):
    splits = date.split('-')    
    day = splits[2]
    return day

def get_month(date):
    return date.split('-')[1]

def get_year(date):
    return date.split('-')[0]   
# Application des fonctions
days = df['Date'].apply(get_day)
months = df['Date'].apply(get_month)
years = df['Date'].apply(get_year)
# Création des nouvelles colonnes
df['day'] = days
df['month'] = months
df['year'] = years
df['year_month']= years+"-"+months
# changement de type de donnée des colonnes month, day, et year
df = df.astype({'year':'int64','month':'int64','day':'int64'})

""">L'Australie étant un pays immense pays, le climat varie d'une région à une autre Pour faciliter l'interprétation des données il parait important d'ajouter une variable "state" au dataset qui permettra de regrouper les localités en 8 zones géographiques.
>Nous ajoutons également les coordonnées géographiques de chaque ville afin de pouvoir les représenter
sur une carte.
"""

#importation du fichier CSV contenant les variables qui nous intéressent
states = pd.read_csv('australian_states.csv',';',index_col=0)
#fusion des deux DataFrames
df = pd.merge(df,states)
#on fait pareil avec le fichier csv que nous avons crée contenant les coordonées géo des villes
lat_long = pd.read_csv("geocoordonnees.csv",index_col=0)
df = pd.merge(df,lat_long)

#afin de faciliter la lecture de df nous allons l'afficher en 2 fois
display(df.iloc[:,0:14].head())
display(df.iloc[:,14:].head())

"""### Cartographie de la variable "Location""""

#création d'une carte avec la position de chaque point de la variable "Location" du dataset

#importation du fichier CSV contenant les données géographiques des villes du Dataset "weatherAUS".
#les variables latitude et longitude ont été récupérées en connectant à l'API de googlemap
df_ville = pd.read_csv("geocoordonnees.csv",index_col=0)

#définition des variables correspondant à la latitude et la longitude de l'Australie
lat = -27.157944693150345
lng = 133.55059052037544

map_options = GMapOptions(lat=lat, lng=lng, map_type="terrain", zoom=4)

p =gmap("AIzaSyC989eZT8qV1z5p3LqYpGa1KkwuqCLucJM", map_options, title="Localisation des différentes ville du dataset")

source = ColumnDataSource(data=df_ville)

c= p.circle(x='Longitude', y='Latitude', size=8, fill_color="red", fill_alpha=0.8,source=source)

#faire apparaitre le nom de la ville lorsque que le curseur passe au dessus d'un point
tooltips = "@Location"
hover = HoverTool(tooltips = tooltips, renderers = [c])

p.add_tools(hover)

show(p)

"""### Analyse de la corrélation des variables numériques"""

#remplacement des modalités des variables 'RainTomorrow' et 'RainToday' par 1 ou 0
df2=pd.read_csv("W_Aus_Na_mean.csv",index_col=0)
df2['RainTomorrow']=df2['RainTomorrow'].replace({'No':0,'Yes':1})
df2['RainToday']=df2['RainToday'].replace({'No':0,'Yes':1})

# création d'une liste des variable catégorielle
l = []
for i in df2.columns:
    if df2.dtypes[i]=='O':
        l.append(i)
# encoder les variables catégorielle avec la classe LebelEncoder
la = LabelEncoder()
for i in l:
    df2[i] = la.fit_transform(df2[i])

# Correlations entre variables numériques
data = round(df2.iloc[:,1:23].corr().abs(),2) #on ne représente pas les variables que nous avons créées (State,day,year...ni date)
#affichage dans une heatmap de la matrice de corrélation
fig, ax = plt.subplots(figsize=(17,13))
sns.heatmap(data,ax=ax,annot = True, cmap = "Spectral" );

# Tri par ordre décroissant des valeurs aboslues du coefficient de pearson
related = data['RainTomorrow'].sort_values(ascending = False)
related

""">Plusieurs variables présentent une corrélation linéaire significative avec la variable cible “RainTomorrow”,  
telle que “Humidity3pm”,”Sunshine” ou encore les variables “Cloud3pm” et “Cloud9am”.

### Distribution de la variable cible et des variables catégorielles
"""

# Aperçu de l'équilibre des classes
df["RainTomorrow"].value_counts(normalize = True)*100

# Répartition de la variable cible RainTomorrow
fig = plt.figure()
ax = fig.add_subplot(111)
sns.countplot(ax = ax, x = df["RainTomorrow"])
plt.title('Distribution de la variable RainTomorrow')
plt.show();

""">Il y a un déséquilibre de classe important de la variable cible RainTomorrow, qui dénombre 78% de jours   sans pluie contre 22% de jours de pluie soit quasiment un rapport de 1 pour 4. Dans la mise en   place du modèle de prédiction il faudra prendre en compte ce déséquilibre, pour éviter la   surreprésentation d’une classe par rapport à l’autre dans les prédictions.  """

#Répartion de RainTomorrow en fonction de RainToday
fig = plt.figure()
ax = fig.add_subplot(111)
sns.countplot(df.RainTomorrow,ax=ax,hue = df.RainToday)
plt.title('Distribution de la variable RainTomorrow en fonction de RainToday')
plt.show()

""">Lorsqu'il pleut le jour J (RainToday), la probabilité qu'il pleuve le lendemain (RainTomorrow) est beaucoup plus importante."""

df_frequency = pd.DataFrame(df.groupby(['State','RainTomorrow']).size())
df_frequency.reset_index(inplace=True)

#calcul de la fréquence de la variable RainTomorrow par état
df_frequency = pd.DataFrame(df.groupby(['State','RainTomorrow']).size())
df_frequency.reset_index(inplace=True)
df_frequency['freq']=1
for s in (df_frequency['State']):
    for r,i in zip (df_frequency[df_frequency['State']==s]['RainTomorrow'],list(df_frequency[df_frequency['State']==s].index)):
        f = df_frequency[df_frequency['State']==s][0].sum()
        df_frequency.iloc[i,3]= df_frequency[(df_frequency['State']==s)&(df_frequency['RainTomorrow']==r)][0]/f

#représentation graphique de la fréquence de la variable raintomorrow par état
f = plt.figure(figsize=(16, 9.5))
gs = f.add_gridspec(2, 4)
ax = f.add_subplot(gs[0, 0])
sns.barplot(x='RainTomorrow',y='freq',data=df_frequency[df_frequency['State']=="South Australia"])
plt.title('South Australia')
plt.ylim(0,0.9)
ax = f.add_subplot(gs[0, 1])
sns.barplot(x='RainTomorrow',y='freq',data=df_frequency[df_frequency['State']=="New South Wales"])
plt.title('New South Wales')
plt.ylim(0,0.9)
ax = f.add_subplot(gs[0, 2])
sns.barplot(x='RainTomorrow',y='freq',data=df_frequency[df_frequency['State']=="NorfolkIsland"])
plt.title('NorfolkIsland')
plt.ylim(0,0.9)
ax = f.add_subplot(gs[0, 3])
sns.barplot(x='RainTomorrow',y='freq',data=df_frequency[df_frequency['State']=="Northern Territory"])
plt.title('Northern Territory')
plt.ylim(0,0.9)
ax = f.add_subplot(gs[1, 0])
sns.barplot(x='RainTomorrow',y='freq',data=df_frequency[df_frequency['State']=="Queensland"])
plt.title('Queensland')
plt.ylim(0,0.9)
ax = f.add_subplot(gs[1, 1])
sns.barplot(x='RainTomorrow',y='freq',data=df_frequency[df_frequency['State']=="Tasmanie"])
plt.title('Tasmanie')
plt.ylim(0,0.9)
ax = f.add_subplot(gs[1, 2])
sns.barplot(x='RainTomorrow',y='freq',data=df_frequency[df_frequency['State']=="Victoria"])
plt.title('Victoria')
plt.ylim(0,0.9)
ax = f.add_subplot(gs[1, 3])
sns.barplot(x='RainTomorrow',y='freq',data=df_frequency[df_frequency['State']=="Western Australia"])
plt.title('Western Australia')
plt.ylim(0,0.9)
plt.show()

""">La distribution de la variable cible entre les différents États d’Australie est relativement homogène."""

# Création d'un countplot avec par variable catégorielle ["WindGustDir", "WindDir9am", "WindDir3pm", "Cloud9am", "Cloud3pm"]
# en appliquant en discriminant avec la variable RainTomorrow
f = plt.figure(figsize=(10, 10))
gs = f.add_gridspec(3, 2)
ax = f.add_subplot(gs[0, 0])
sns.countplot(df.WindGustDir,ax=ax, palette = 'colorblind', hue = df.RainTomorrow)
plt.xticks(size = 6)
ax = f.add_subplot(gs[0, 1])
sns.countplot(df.WindDir9am,ax=ax,palette = 'colorblind', hue = df.RainTomorrow)
plt.xticks(size = 6)
ax = f.add_subplot(gs[1, 0])
sns.countplot(df.WindDir3pm,ax=ax,palette = 'colorblind', hue = df.RainTomorrow)
plt.xticks(size = 6)
ax = f.add_subplot(gs[1, 1])
sns.countplot(df.Cloud9am,ax=ax, palette = 'colorblind', hue = df.RainTomorrow)
plt.xticks(size = 6)
ax = f.add_subplot(gs[2, 0])
sns.countplot(df.Cloud3pm,ax=ax, palette = 'colorblind', hue = df.RainTomorrow)
plt.xticks(size = 6)
plt.show()

""">La distribution des variables liées à la direction du vent sont relativement homogène à l’exception du vent du Nord de la variable “WindDir9am” qui semble plus fortement corrélé au fait qu’il pleuve le lendemain (“RainTomorrow” = yes).  
>En revanche les variables “Cloud3pm” et “Cloud9am” sont étroitement liées au temps qu’il fera le lendemain, car moins le temps est nuageux moins il y a de risque de pluie le lendemain.
"""

#Moyenne des chutes de pluie par an et par Etat
sns.relplot(df.year,df.Rainfall,hue = df.State, ci = None,kind = 'line',height=6,aspect=2)
plt.ylabel('rainfall en mm')
plt.title('Chutes de pluie par Etat')
plt.show()

"""La distribution des variables liées à la direction du vent sont relativement homogène à l’exception du vent du Nord de la variable “WindDir9am” qui semble plus fortement corrélé au fait qu’il pleuve le lendemain (“RainTomorrow” = yes).
En revanche les variables “Cloud3pm” et “Cloud9am” sont étroitement liées au temps qu’il fera le lendemain, car moins le temps est nuageux moins il y a de risque de pluie le lendemain.

"""

#pluie en fonction des mois puis de l'année
g=sns.relplot(df.year_month,df.Rainfall,col = df.State,col_wrap=2, ci = None,kind = 'line',height=2,aspect=4)
g.set_xlabels('de 2007 à 2017')

plt.xticks("")

plt.show()

fig = plt.figure(figsize=(35,10))
ax = fig.add_subplot(111)
sns.lineplot(ax = ax, x = 'year_month', y = 'MinTemp',ci=None, data = df,label='MinTemps')
sns.lineplot(ax = ax, x = 'year_month', y = 'MaxTemp',ci=None, data = df,label='MinTemps')
plt.title("Evolution de des températures minimales et maximales")

plt.xlabel("")
#Rotation des dates
plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")


plt.show()

"""### Identification des outliers"""

fig = plt.figure(figsize=(20, 17))
gs = fig.add_gridspec(2, 2)

ax = fig.add_subplot(gs[0, 0])
sns.boxplot(y='MaxTemp',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20)

ax = fig.add_subplot(gs[0, 1])
sns.boxplot(y='MinTemp',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20)

ax = fig.add_subplot(gs[1, 0])
sns.boxplot(y='Temp9am',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20)

ax = fig.add_subplot(gs[1, 1])
sns.boxplot(y='Temp3pm',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20);

fig = plt.figure(figsize=(20, 25))
gs = fig.add_gridspec(4, 3)
ax = fig.add_subplot(gs[0, 0])
sns.boxplot(y='Humidity3pm',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20)

ax = fig.add_subplot(gs[0, 1])
sns.boxplot(y='Cloud9am',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20)

ax = fig.add_subplot(gs[0, 2])
sns.boxplot(y='Cloud3pm',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20)

ax = fig.add_subplot(gs[1, 0])
sns.boxplot(y='Rainfall',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20);

ax = fig.add_subplot(gs[1, 1])
sns.boxplot(y='Evaporation',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20)

ax = fig.add_subplot(gs[1, 2])
sns.boxplot(y='Humidity9am',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20)

ax = fig.add_subplot(gs[2, 0])
sns.boxplot(y='Pressure9am',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20)

ax = fig.add_subplot(gs[2, 1])
sns.boxplot(y='Pressure3pm',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20)

ax = fig.add_subplot(gs[3, 0])
sns.boxplot(y='WindGustSpeed',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20)

ax = fig.add_subplot(gs[3, 1])
sns.boxplot(y='WindSpeed9am',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20)

ax = fig.add_subplot(gs[3, 2])
sns.boxplot(y='WindSpeed3pm',x='State',data=df,ax=ax)
sns.despine(top = True, bottom = True, left = False, right = False)
plt.xticks(rotation=20)
plt.show()

""">Il y a de nombreuses valeurs extrêmes pour plusieurs variables, mais il est difficile de distinguer celles qui sont issues d’un évènement climatique isolé, à celles qui sont liées à des évènements climatiques cycliques tels que El Niño et La Ninã.  
Nous décidons donc de conserver dans un premier temps ces valeurs et reviendrons sur ce point selon les résultats de nos modèles prédictifs.

# 1 - Prepocessing

### 1.1.1 - Méthode KNNImputer
Attention, le remplacement des valeurs manquantes par cette technique pourra avoir des conséquences sur notre modèle prédictif notamment en terme d'overfitting.
"""

# On ne garde dans un premier temps que les données étiquetées
df_knn = df[(df["RainTomorrow"] == "No") | (df["RainTomorrow"] == "Yes") ]
#suppression des variables pour lesquelles un trop grand nombre valeurs est manquant
df_knn.drop(['Evaporation','Sunshine','Cloud3pm'],axis=1,inplace=True)

"""Préalablement au traitement des valeurs manquantes, il faut supprimer les outliers qui peuvent avoir un impact sur les valeurs qui seront imputées.  
Pour l’identification des outliers nous avons utilisé la méthode Inter Quartile Range (IQR) qui se calcul de la manière suivante:  
IQR = Q3 - Q1  
ou :  
Q1 est défini comme le nombre intermédiaire entre le plus petit nombre et la médiane de l’ensemble de données.  
Q3 est la valeur médiane entre la médiane et la valeur la plus élevée de l’ensemble de données.  
Avec l’IQR nous pouvons fixer la limite supérieure et inférieure d’un ensemble de valeurs pour une variable, de la manière suivante:  
supérieur = Q3 +IQR  
inférieur = Q1 – IQR  

"""

#avant d'utiliser l'outil knnImputer, nous supprimons les valeurs extrêmes car elles ont auront une incidence sur les valeurs imputées
state = list(df.State.value_counts().index)
variables = ['MinTemp','MaxTemp','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','Temp9am','Temp3pm','Cloud9am']
#Création du dataframe df2 dans lequel nous supprimons les valeurs manquantes de ['MaxTemp','MinTemp','Temp9am','Temp3pm'] pour que le calcul puisse se faire
df2=df_knn
for s in state:
    for var in variables:
        Q1 = np.percentile(df2[df2['State']==s][var], 25,interpolation = 'midpoint')
        Q3 = np.percentile(df2[df2['State']==s][var], 75,interpolation = 'midpoint')
        IQR = Q3 - Q1
        up = np.where(df2[df2['State']==s][var] >= (Q3-IQR))
        low = np.where(df2[df2['State']==s][var] <= (Q1-IQR))
        df_knn.drop(up[0], inplace = True,errors='ignore')
        df_knn.drop(low[0], inplace = True,errors='ignore')
#on reconstitu l'index car celui-ci n'est plus continu
df_knn = df_knn.reset_index()

#création de la liste des variable que nous allons encoder
l = ['WindGustDir','WindDir9am','WindDir3pm']

# encoder les variables catégorielle avec la classe LebelEncoder
la = LabelEncoder()
for i in l:
    df_knn[i] = la.fit_transform(df_knn[i])

#création d'une liste contenant les noms de colonnes qui nous servira à renommer les colonnes à l'issue du processus de knnImputer
columns_name = list(df_knn.select_dtypes(exclude = 'object').columns)
#sauvegarde d'un dataframe ne contenant que les variable de type objet, afin de reconstituer le dataset à l'issue du processus de knnImputer
df_knn_obj = df_knn.drop(columns_name,axis=1)
# puis nous exlcluons les variables de type 'object qu'il n'était pas pertinent d'encoder
df_knn_num = df_knn.select_dtypes(exclude = 'object')
df_knn_num.drop(columns='index',inplace=True)

# importation de la classe KNNImputer
from sklearn.impute import KNNImputer
# création du classifieur
imputer = KNNImputer(n_neighbors=2)
# entrainement du classifieur sur df_knn
df_knn3 = imputer.fit_transform(df_knn_num)

#création à partir de l'array
df_knn2 = pd.DataFrame(df_knn3)

# instanciation d'un dictionnaire qui contiendra le nom actuel des variables et le nom d'origine que nous vous réimputer
dico = {}
for n, e in zip (range (0,21),columns_name):
    dico[n]=e
#renommer les colonnes avec le nom des variables d'origine
df_knn2.rename(columns=dico,inplace=True)

# fusion df_knn et df_knn2
df_knn2 = pd.concat([df_knn2,df_knn_obj],axis=1)

#l'algorytme knn fonctionne uniquement pour les variables continues. Pour les variables catégorielles nous imputerons la modalités la plus fréquente de la variable en fonction de la variable 'State'
df_1 = df_knn2[df_knn2['State']=='New South Wales']
for col in (df_1.columns):
  df_1[col].fillna(df_1[col].mode()[0],inplace=True)
df_2 = df_knn2[df_knn2['State']=='Victoria']
for col in (df_2.columns):
  df_2[col].fillna(df_2[col].mode()[0],inplace=True)
df_3 = df_knn2[df_knn2['State']=='Western Australia']
for col in (df_3.columns):
  df_3[col].fillna(df_3[col].mode()[0],inplace=True)
df_4 = df_knn2[df_knn2['State']=='Queensland']
for col in (df_4.columns):
  df_4[col].fillna(df_4[col].mode()[0],inplace=True)
df_5 = df_knn2[df_knn2['State']=='South Australia']
for col in (df_5.columns):
  df_5[col].fillna(df_5[col].mode()[0],inplace=True)
df_6 = df_knn2[df_knn2['State']=='Northern Territory']
for col in (df_6.columns):
  df_6[col].fillna(df_6[col].mode()[0],inplace=True)
df_7 = df_knn2[df_knn2['State']=='Tasmanie']
for col in (df_7.columns):
  df_7[col].fillna(df_7[col].mode()[0],inplace=True)
df_8 = df_knn2[df_knn2['State']=='NorfolkIsland']
for col in (df_8.columns):
  df_8[col].fillna(df_8[col].mode()[0],inplace=True)

#Nous concaténons l'ensemble des sous ensemble et nous vérifion qu'il n'y a plus de valeurs manquantes
df_knnvf = pd.concat([df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8],axis=0)

#écriture d'un fichier csv à partir de df
df_knnvf.to_csv('W_Aus_Na_knn.csv')

"""### 1.1.2 - Imputation de la moyenne ou de la modalité la plus fréquente
Dans cette méthode, nous imputons aux valeurs manquantes la moyennes de chaque variable numérique en fonction de la variable 'State'.
Et nous imputons la modalités la plus fréquente de chaque variable catégorielle en fonction de la variable 'State'.
"""

# On défini un nouveau Dataframe en ne gardant dans un premier temps que les données étiquetées
df_mean = df[(df["RainTomorrow"] == "No") | (df["RainTomorrow"] == "Yes") ]
#suppression des variables pour lesquelles un trop grand nombre valeurs est manquant
df_mean.drop(['Evaporation','Sunshine','Cloud3pm'],axis=1,inplace=True)

"""Préalablement au traitement des valeurs manquantes, il faut supprimer les outliers qui peuvent avoir un impact sur les valeurs qui seront imputées.  
Pour l’identification des outliers nous avons utilisé la méthode Inter Quartile Range (IQR) qui se calcul de la manière suivante:  
IQR = Q3 - Q1  
ou :  
Q1 est défini comme le nombre intermédiaire entre le plus petit nombre et la médiane de l’ensemble de données.  
Q3 est la valeur médiane entre la médiane et la valeur la plus élevée de l’ensemble de données.  
Avec l’IQR nous pouvons fixer la limite supérieure et inférieure d’un ensemble de valeurs pour une variable, de la manière suivante:  
supérieur = Q3 +IQR  
inférieur = Q1 – IQR  

"""

#avant d'appliquer la moyenne nous supprimons les valeurs extrêmes car elles ont auront une incidence sur les valeurs imputées
state = list(df.State.value_counts().index)
variables = ['MinTemp','MaxTemp','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','Temp9am','Temp3pm','Cloud9am']
df2=df_mean
for s in state:
    for var in variables:
        Q1 = np.percentile(df2[df2['State']==s][var], 25,interpolation = 'midpoint')
        Q3 = np.percentile(df2[df2['State']==s][var], 75,interpolation = 'midpoint')
        IQR = Q3 - Q1
        up = np.where(df2[df2['State']==s][var] >= (Q3+IQR))
        low = np.where(df2[df2['State']==s][var] <= (Q1-IQR))
        df_mean.drop(up[0], inplace = True,errors='ignore')
        df_mean.drop(low[0], inplace = True,errors='ignore')

df_1 = df_mean[df_mean['State']=='New South Wales']
df_1.fillna(df_1.mean(),inplace = True)
for col in (df_1.columns):
  df_1[col].fillna(df_1[col].mode()[0],inplace=True)

df_2 = df_mean[df_mean['State']=='Victoria']
df_2.fillna(df_2.mean(),inplace = True)
for col in (df_2.columns):
  df_2[col].fillna(df_2[col].mode()[0],inplace=True)

df_3 = df_mean[df_mean['State']=='Western Australia']
df_3.fillna(df_3.mean(),inplace = True)
for col in (df_3.columns):
  df_3[col].fillna(df_3[col].mode()[0],inplace=True)
  
df_4 = df_mean[df_mean['State']=='Queensland']
df_4.fillna(df_4.mean(),inplace = True)
for col in (df_4.columns):
  df_4[col].fillna(df_4[col].mode()[0],inplace=True)

df_5 = df_mean[df_mean['State']=='South Australia']
df_5.fillna(df_5.mean(),inplace = True)
for col in (df_5.columns):
  df_5[col].fillna(df_5[col].mode()[0],inplace=True)

df_6 = df_mean[df_mean['State']=='Northern Territory']
df_6.fillna(df_6.mean(),inplace = True)
for col in (df_6.columns):
  df_6[col].fillna(df_6[col].mode()[0],inplace=True)

df_7 = df_mean[df_mean['State']=='Tasmanie']
df_7.fillna(df_7.mean(),inplace = True)
for col in (df_7.columns):
  df_7[col].fillna(df_7[col].mode()[0],inplace=True)

df_8 = df_mean[df_mean['State']=='NorfolkIsland']
df_8.fillna(df_8.mean(),inplace = True)
for col in (df_8.columns):
  df_8[col].fillna(df_8[col].mode()[0],inplace=True)

#Nous concaténons l'ensemble des sous ensemble et nous vérifions qu'il n'y a plus de valeurs manquantes
df_mean = pd.concat([df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8],axis=0)
df_mean.info()

#écriture d'un fichier csv à partir de df
df_mean.to_csv('W_Aus_Na_mean.csv')

"""## 1.2 - Preparation des données (avec NaN mean)"""

#importation du fichier dont la moyenne a été imputée aux valeur Na
df_mean = pd.read_csv('W_Aus_Na_mean.csv', index_col = 0)

#remplacement des modalités des variables 'RainTomorrow' et 'RainToday' par 1 ou 0
df_mean['RainTomorrow']=df_mean['RainTomorrow'].replace({'No':0,'Yes':1})
df_mean['RainToday']=df_mean['RainToday'].replace({'No':0,'Yes':1})

#suppression de la colonne 'Date' qui n'a pas d'incidence fondamentale sur le résultat final
df_mean.drop(['Location','year','day','State'],axis=1,inplace=True)

# création des DataFrame features et target
features_mean = df_mean.drop('RainTomorrow',axis=1)
target_mean = df_mean['RainTomorrow']


# création d'une liste des variable catégorielle
l = []
for i in features_mean.columns:
    if features_mean.dtypes[i]=='O':
        l.append(i)
# encoder les variables catégorielle avec la classe LebelEncoder
la = LabelEncoder()
for i in l:
    features_mean[i] = la.fit_transform(features_mean[i])

# Centrer et réduire les variables numériques
scaler = StandardScaler()
features_mean = scaler.fit_transform(features_mean)

# création d'un jeu d'entrainement et de test sans traiter le déséquilibre des classes
X_train_m,X_test_m,y_train_m,y_test_m = train_test_split(features_mean,target_mean,test_size=0.2,random_state=789)

bal = SMOTE()
X_train_msm, y_train_msm = bal.fit_resample(X_train_m, y_train_m)

"""## 1.3 - Preparation des données (avec NaN knnImputer)"""

#importation du fichier dont les valeurs manquantes ont été traitée avec knn_imputer
df_knn = pd.read_csv('W_Aus_Na_knn.csv', index_col = 0)

#remplacement des modalités des variables 'RainTomorrow' et 'RainToday' par 1 ou 0
df_knn['RainTomorrow']=df_knn['RainTomorrow'].replace({'No':0,'Yes':1})
df_knn['RainToday']=df_knn['RainToday'].replace({'No':0,'Yes':1})

#suppression de la colonne 'Date' qui n'a pas d'incidence fondamentale sur le résultat final
df_knn.drop(['Date','year_month','State'],axis=1,inplace=True)

# création des DataFrame features et target
features_knn = df_knn.drop('RainTomorrow',axis=1)
target_knn = df_knn['RainTomorrow']

# création d'une liste des variable catégorielle
l = []
for i in features_knn.columns:
    if features_knn.dtypes[i]=='O':
        l.append(i)
# encoder les variables catégorielle avec la classe LebelEncoder
la = LabelEncoder()
for i in l:
    features_knn[i] = la.fit_transform(features_knn[i])

# Centrer et réduire les variables numériques
scaler = StandardScaler()
features_knn = scaler.fit_transform(features_knn)

# création d'un jeu d'entrainement et de test sans traiter le déséquilibre des classes
X_train_k,X_test_k,y_train_k,y_test_k = train_test_split(features_knn,target_knn,test_size=0.2,random_state=789)

# rééquilibrage des classe en utilisant la méthode d'oversampling avec la classe SMOTE
bal = SMOTE()
X_train_ksm, y_train_ksm = bal.fit_resample(X_train_k, y_train_k)

"""# 2 - MODELISATION

## 2.1 - Modele 1 : Regression Logistique

### 2.1.1 - Avec NaN mean
"""

# Entrainement modèle 
lr = LogisticRegression(class_weight = 'balanced',C=1 )
lr.fit(X_train_m, y_train_m)

# Performances

# Score d'accuracy globale
print("train score :", lr.score(X_train_msm, y_train_msm))
print("test score :", lr.score(X_test_m,y_test_m))

# Matrice de confusion
y_pred = lr.predict(X_test_m)
cm = pd.crosstab(y_test_m, y_pred, rownames = ['Real'], colnames = ['Pred'])
display(cm)

# Autres métriques
print(classification_report_imbalanced(y_test_m, y_pred))

# ROC_curves
probs = lr.predict_proba(X_test_m)
fpr,tpr,seuils = roc_curve(y_test_m, probs[:,1], pos_label = 1)
roc_auc = auc(fpr,tpr)

print(seuils)
print(roc_auc)

fig = plt.figure(figsize=(6,4))

ax = fig.add_subplot(111)

ax.plot(fpr,tpr, 'y-', label = ('Modèle lr (auc = ' + str(round(roc_auc,2)) + ')'))
ax.plot([0,1],[0,1], 'b--', label = 'Aléatoire (auc = 0.5)')
ax.set_xlim(0,1)
ax.set_ylim(0,1)
ax.set_xlabel("Taux faux positifs")
ax.set_ylabel("Taux vrais positifs")
ax.set_title("Courbe ROC")
ax.legend(loc = "lower right");

# Tunning de l'hyperparamètre C

C_param_range = {'C':[0.001,0.01,0.1,1,10,100]}

lr = LogisticRegression(max_iter= 500, class_weight = 'balanced' )

grclf = GridSearchCV(estimator = lr,
                     param_grid = C_param_range)

grclf.fit(X_train_m, y_train_m)

grclf.best_params_

"""### 2.1.2 - Avec NaN knnImputer"""

# Entrainement modèle 
lr = LogisticRegression(class_weight = 'balanced', C = 1 )
lr.fit(X_train_k, y_train_k)

# Performances

# Score d'accuracy globale
print("train score :", lr.score(X_train_k, y_train_k))
print("test score :", lr.score(X_test_k,y_test_k))

# Matrice de confusion
y_pred = lr.predict(X_test_k)
cm = pd.crosstab(y_test_k, y_pred, rownames = ['Real'], colnames = ['Pred'])
display(cm)

# Autres métriques
print(classification_report_imbalanced(y_test_k, y_pred))

# ROC_curves
probs = lr.predict_proba(X_test_k)
fpr,tpr,seuils = roc_curve(y_test_k, probs[:,1], pos_label = 1)
roc_auc = auc(fpr,tpr)

print(seuils)
print(roc_auc)

fig = plt.figure(figsize=(6,4))

ax = fig.add_subplot(111)

ax.plot(fpr,tpr, 'y-', label = ('Modèle lr (auc = ' + str(round(roc_auc,2)) + ')'))
ax.plot([0,1],[0,1], 'b--', label = 'Aléatoire (auc = 0.5)')
ax.set_xlim(0,1)
ax.set_ylim(0,1)
ax.set_xlabel("Taux faux positifs")
ax.set_ylabel("Taux vrais positifs")
ax.set_title("Courbe ROC")
ax.legend(loc = "lower right");

"""## 2.2 - Classification Knn

### 2.2.1 - Avec NaN mean
"""

from sklearn import neighbors
clf_knn = neighbors.KNeighborsClassifier(n_neighbors = 7, metric='minkowski' )
clf_knn.fit(X_train_msm, y_train_msm)

# Performances

# Score d'accuracy globale
print("trains score",clf_knn.score(X_train_m,y_train_m))
print("test score :", clf_knn.score(X_test_m,y_test_m))
# Matrice de confusion
y_pred = clf_knn.predict(X_test_m)
cm = pd.crosstab(y_test_m, y_pred, rownames = ['Real'], colnames = ['Pred'])
display(cm)

# Autres métriques
print(classification_report_imbalanced(y_test_m, y_pred))

"""### 2.2.2 - Avec NaN knnImputer"""

from sklearn import neighbors
clf_knn = neighbors.KNeighborsClassifier(n_neighbors = 7, metric='minkowski' )
clf_knn.fit(X_train_ksm, y_train_ksm)

# Performances

# Score d'accuracy globale
print("train score :", clf_knn.score(X_train_k,y_train_k))
print("test score :", clf_knn.score(X_test_k,y_test_k))

# Matrice de confusion
y_pred = clf_knn.predict(X_test_k)
cm = pd.crosstab(y_test_k, y_pred, rownames = ['Real'], colnames = ['Pred'])
display(cm)

# Autres métriques
print(classification_report_imbalanced(y_test_k, y_pred))

# ROC_curves
probs = clf_knn.predict_proba(X_test_k)
fpr,tpr,seuils = roc_curve(y_test_k, probs[:,1], pos_label = 1)
roc_auc = auc(fpr,tpr)

print(seuils)
print(roc_auc)

fig = plt.figure(figsize=(6,4))

ax = fig.add_subplot(111)

ax.plot(fpr,tpr, 'y-', label = ('Modèle lr (auc = ' + str(round(roc_auc,2)) + ')'))
ax.plot([0,1],[0,1], 'b--', label = 'Aléatoire (auc = 0.5)')
ax.set_xlim(0,1)
ax.set_ylim(0,1)
ax.set_xlabel("Taux faux positifs")
ax.set_ylabel("Taux vrais positifs")
ax.set_title("Courbe ROC")
ax.legend(loc = "lower right");

"""## 2.3 - La classification avec SVM

### 2.3.1 - Avec NaN mean
"""

# Entrainement modèle 
clf_svc = SVC(class_weight='balanced')
clf_svc.fit(X_train_m, y_train_m)

# Performances

# Score d'accuracy globale
print("train score :", clf_svc.score(X_train_m, y_train_m))
print("test score :", clf_svc.score(X_test_m,y_test_m))

# Matrice de confusion
y_pred = clf_svc.predict(X_test_m)
cm = pd.crosstab(y_test_m, y_pred, rownames = ['Real'], colnames = ['Pred'])
display(cm)

# Autres métriques
print(classification_report_imbalanced(y_test_m, y_pred))

# Avec noyau 'rbf'
clf_svc = SVC(kernel = 'rbf', class_weight='balanced')
clf_svc.fit(X_train_m, y_train_m)

# Performances

# Score d'accuracy globale
print("train score :", clf_svc.score(X_train_m, y_train_m))
print("test score :", clf_svc.score(X_test_m,y_test_m))

# Matrice de confusion
y_pred = clf_svc.predict(X_test_m)
cm = pd.crosstab(y_test_m, y_pred, rownames = ['Real'], colnames = ['Pred'])
display(cm)

# Autres métriques
print(classification_report_imbalanced(y_test_m, y_pred))

"""### 2.3.2 - Avec NaN KnnImputer"""

# Entrainement modèle 
clf_svc = SVC(class_weight='balanced')
clf_svc.fit(X_train_k, y_train_k)

# Performances

# Score d'accuracy globale
print("train score :", clf_svc.score(X_train_k, y_train_k))
print("test score :", clf_svc.score(X_test_k,y_test_k))

# Matrice de confusion
y_pred = clf_svc.predict(X_test_k)
cm = pd.crosstab(y_test_k, y_pred, rownames = ['Real'], colnames = ['Pred'])
display(cm)

# Autres métriques
print(classification_report_imbalanced(y_test_k, y_pred))

"""## 2.4 - La classification avec RandomForest

### 2.4.1 - Avec NaN mean
"""

#afin de déterminer les meilleurs paramètres à utiliser dans notre modèle RandomForest nous réalisons un GrisearchCV 
clf_rf = RandomForestClassifier (random_state=789)

param_grid_rf = [{'min_samples_leaf': [1, 3, 5],
                        'max_features': ['sqrt', 'log2']}]
gridcv = GridSearchCV(estimator = clf_rf, param_grid = param_grid_rf,scoring='accuracy',cv=3)
gridcv.fit(X_train_msm,y_train_msm)

#affichage des résultats dans un DataFrame
pd.DataFrame(gridcv.cv_results_)[['params', 'mean_test_score','std_test_score']]

#les paramètres retenu sont max_features = 'sqrt' et min_samples_leaf = 1

clf_rf_m = RandomForestClassifier (max_features = 'sqrt',min_samples_leaf = 1,class_weight="balanced", random_state=789)
clf_rf_m.fit(X_train_msm,y_train_msm)
y_pred_m = clf_rf_m.predict(X_test_m)

# Calcul du score de train et test
print("train score :", clf_rf_m.score(X_train_m, y_train_m))
print("test score :", clf_rf_m.score(X_test_m,y_test_m))

# affichage de la matrice de confusion
cm = pd.crosstab(y_test_m, y_pred_m, rownames = ['Real'], colnames = ['Pred'])
display(cm)

# Autres métriques
print(classification_report_imbalanced(y_test_m, y_pred_m))

"""### 2.4.2 - Avec NaN knnImputer"""

clf_rf_knn = RandomForestClassifier (max_features = 'sqrt',min_samples_leaf = 1,class_weight="balanced", random_state=789)
clf_rf_knn.fit(X_train_k,y_train_k)
y_pred_knn = clf_rf_knn.predict(X_test_k)

# Calcul du score de train et test
print("train score :", clf_rf_knn.score(X_train_k, y_train_k))
print("test score :", clf_rf_knn.score(X_test_k,y_test_k))

# affichage de la matrice de confusion
cm = pd.crosstab(y_test_k, y_pred_knn, rownames = ['Real'], colnames = ['Pred'])
display(cm)

# Autres métriques
print(classification_report_imbalanced(y_test_k, y_pred_knn))

"""## 2.5 -  La classification avec GradientBoostingClassifier

### 2.5.2 - Avec NaN mean
"""

#On détermine les meilleurs paramètres avec GridSeachCV
#instanciation d'un classifieur
gbcl = GradientBoostingClassifier(random_state=789, loss ='deviance',subsample = 0.5, max_depth=15)

#dictionnaire de paramètre
param_grid_gbcl = [{'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001], 'n_estimators':[100,250,500,750,1000,1250,1500,1750]}]
#instanciation de GrisdSearchCV
gridcv = GridSearchCV(estimator = gbcl, param_grid = param_grid_gbcl,scoring='accuracy',cv=5)
#entrainement
gridcv.fit(X_train_msm,y_train_msm)
#affichage des résultat obtenu dans un DataFrame
pd.DataFrame(gridcv.cv_results_)[['params', 'mean_test_score','std_test_score']]

#instanciation d'un classifieur avec les meilleurs paramètres retenus
gbcl = GradientBoostingClassifier(random_state=789, loss ='deviance',subsample = 0.5, max_depth=15,n_estimators=1000,learning_rate=0.1)
gbcl.fit(X_train_msm,y_train_msm)
y_pred_gbcl = gbcl.predict(X_test_m)

# Calcul du score de train et test
print("train score :", gbcl.score(X_train_m,y_train_m))
print("test score :", gbcl.score(X_test_m, y_test_m))

# affichage de la matrice de confusion
cm = pd.crosstab(y_test_m, y_pred_gbcl, rownames = ['Real'], colnames = ['Pred'])
display(cm)

# Autres métriques
print(classification_report_imbalanced(y_test_m, y_pred_gbcl))

"""### 2.5.2 - Avec NaN knnImputer"""

#instanciation d'un classifieur avec les meilleurs paramètres retenus
gbcl = GradientBoostingClassifier(random_state=789, loss ='deviance',subsample = 0.5, max_depth=15,n_estimators=1000,learning_rate=0.1)
gbcl.fit(X_train_ksm,y_train_ksm)
y_pred_gbcl = gbcl.predict(X_test_k)

# Calcul du score de train et test
print("train score :", gbcl.score(X_train_k,y_train_k))
print("test score :", gbcl.score(X_test_k, y_test_k))

# affichage de la matrice de confusion
cm = pd.crosstab(y_test_k, y_pred_gbcl, rownames = ['Real'], colnames = ['Pred'])
display(cm)

# Autres métriques
print(classification_report_imbalanced(y_test_k, y_pred_gbcl))

"""## 2.6 -  La classification avec XGboost
Détermination du meilleur réglage des paramètres avec la class GridSearchCV
"""

#1ère itération
#Création d'un dictionnaire contenant les paramètres de base
param = {}
param['booster'] = 'gbtree'
param['objective'] = 'binary:logistic'
param["eval_metric"] = "error"
param['gamma'] = 0
param['max_depth'] = 6
param['min_child_weight']=1
param['max_delta_step'] = 0
param['subsample']= 1
param['colsample_bytree']=1
param['silent'] = 1
param['seed'] = 0
param['base_score'] = 0.5
#instanciation du classifieur xgb
xgb = xgb.XGBClassifier(params=param,random_state=42)

#création du dictionnaire des paramètres avec les valeurs sur lesquelles GridSearchCV va itérer
param_grid_xgb = [{'num_boost_round': [250, 500, 750],
                        'learning_rate': [0.3,0.1,0.05],
                        'eta':[0.2,0.1,0.01],
                        'max_depth':[6,9,12]}]

# instanciation du classifieur gridcv
gridcv = GridSearchCV(estimator = xgb, param_grid = param_grid_xgb,scoring='accuracy',cv=3)
#entrainement sur le train set
gridcv.fit(X_train_msm,y_train_msm)

#affichage des résultats dans un DataFrame
display(pd.DataFrame(gridcv.cv_results_)[['params', 'mean_test_score','std_test_score']].sort_values('mean_test_score',ascending=False).head())

print("Le meilleur réglage des paramètres est le suivant:",gridcv.best_params_)

#2ème itération
#Création d'un dictionnaire contenant les paramètres de base
param = {}
param['booster'] = 'gbtree'
param['objective'] = 'binary:logistic'
param["eval_metric"] = "error"
param['num_boost_round']=250
param['learning_rate ']=0.1
param['gamma'] = 0
param['max_depth'] = 6
param['min_child_weight']=1
param['max_delta_step'] = 0
param['subsample']= 1
param['colsample_bytree']=1
param['silent'] = 1
param['seed'] = 0
param['base_score'] = 0.5
#instanciation du classifieur xgb
xgb = xgb.XGBClassifier(params=param,random_state=42)

#création du dictionnaire des paramètres avec les valeurs sur lesquelles GridSearchCV va itérer
param_grid_xgb = [{'eta':[0.3,0.4,0.5],'max_depth':[15,18,23]}]

# instanciation du classifieur gridcv
gridcv = GridSearchCV(estimator = xgb, param_grid = param_grid_xgb,scoring='accuracy',cv=3)
#entrainement sur le train set
gridcv.fit(X_train_msm,y_train_msm)

#affichage des résultats dans un DataFrame de la 2ème itération
display(pd.DataFrame(gridcv.cv_results_)[['params', 'mean_test_score','std_test_score']].sort_values('mean_test_score',ascending=False).head())

print("Le meilleur réglage des paramètres est le suivant:",gridcv.best_params_)

"""### 2.6.1 - Avec NaN mean"""

import xgboost as xgb



param = {}
param['booster'] = 'gbtree'
param['objective'] = 'binary:logistic'
param['num_boost_round']=250
param['learning_rate ']=0.1
param["eval_metric"] = "error"
param['eta'] = 0.3
param['gamma'] = 1
param['silent']=1
param['max_depth'] = 23
param['min_child_weight']=4
param['max_delta_step'] = 0
param['subsample']= 0.8
param['colsample_bytree']=1
param['silent'] = 1
param['seed'] = 0
param['base_score'] = 0.5


xgb = xgb.XGBClassifier(params=param,random_state=42)
xgb.fit(X_train_msm,y_train_msm)
xgbpreds = xgb.predict(X_test_m)

print("train score :", xgb.score(X_train_m, y_train_m))
print("test score :", xgb.score(X_test_m,y_test_m))
cm = pd.crosstab(y_test_m, xgbpreds, rownames = ['Real'], colnames = ['Pred'])
display(cm)
print(classification_report_imbalanced(y_test_m, xgbpreds))

# ROC_curves
probs = xgb.predict_proba(X_test_m)
fpr,tpr,seuils = roc_curve(y_test_m, probs[:,1], pos_label = 1)
roc_auc = auc(fpr,tpr)


fig = plt.figure(figsize=(6,4))

ax = fig.add_subplot(111)

ax.plot(fpr,tpr, 'y-', label = ('Modèle lr (auc = ' + str(round(roc_auc,2)) + ')'))
ax.plot([0,1],[0,1], 'b--', label = 'Aléatoire (auc = 0.5)')
ax.set_xlim(0,1)
ax.set_ylim(0,1)
ax.set_xlabel("Taux faux positifs")
ax.set_ylabel("Taux vrais positifs")
ax.set_title("Courbe ROC")
ax.legend(loc = "lower right");

"""### 2.6.2 - Avec NaN knnImputer"""

import xgboost as xgb



param = {}
param['booster'] = 'gbtree'
param['objective'] = 'binary:logistic'
param['num_boost_round']=250
param['learning_rate ']=0.1
param["eval_metric"] = "error"
param['eta'] = 0.3
param['gamma'] = 1
param['silent']=1
param['max_depth'] = 23
param['min_child_weight']=4
param['max_delta_step'] = 0
param['subsample']= 0.8
param['colsample_bytree']=1
param['silent'] = 1
param['seed'] = 0
param['base_score'] = 0.5


xgb = xgb.XGBClassifier(params=param,random_state=42)
xgb.fit(X_train_ksm,y_train_ksm)
xgbpreds = xgb.predict(X_test_k)

print("train score :", xgb.score(X_train_k, y_train_k))
print("test score :", xgb.score(X_test_k,y_test_k))
cm = pd.crosstab(y_test_k, xgbpreds, rownames = ['Real'], colnames = ['Pred'])
display(cm)
print(classification_report_imbalanced(y_test_k, xgbpreds))

"""## 2.7 Classification avec réseau de neurones denses"""

from tensorflow.keras.layers import Input, Dense #Pour instancier une couche Dense et une d'Input
from tensorflow.keras.models import Model

"""### 2.7.1 - Avec NaN mean"""

X_train_m.shape
#Les features sont composées de 22 variables, ce que nous mettrons au paramètre shape de Input

inputs = Input(shape = 22,name='Input')
dense1 = Dense(units=20,activation='tanh',name='Dense_1')
dense2 = Dense(units=10,activation='tanh',name='Dense_2')
dense3 = Dense(units = 5, activation = "tanh", name = "Dense_3")
dense4 = Dense(units = 3, activation = "softmax", name = "Dense_4")

x = dense1(inputs)
x = dense2(x)
x = dense3(x)
outputs = dense4(x)

model = Model(inputs = inputs, outputs = outputs)
model.summary()

model.compile(loss = "sparse_categorical_crossentropy",
              optimizer = "adam",
              metrics = ["accuracy"])

model.fit(X_train_msm,y_train_msm,epochs=15,batch_size=15,validation_split=0.1)

y_pred = model.predict(X_test_m)

test_pred_class = np.argmax(y_pred, axis=1)
cm = pd.crosstab(y_test_m, test_pred_class, rownames = ['Real'], colnames = ['Pred'])
display(cm)
print(classification_report_imbalanced(y_test_m, test_pred_class))

"""### 2.7.2 - Avec NaN knnImputer"""

inputs = Input(shape = 23,name='Input')
dense1 = Dense(units=10,activation='tanh',name='Dense_1')
dense2 = Dense(units=8,activation='tanh',name='Dense_2')
dense3 = Dense(units = 6, activation = "tanh", name = "Dense_3")
dense4 = Dense(units = 3, activation = "softmax", name = "Dense_4")

x = dense1(inputs)
x = dense2(x)
x = dense3(x)
outputs = dense4(x)

model_k = Model(inputs = inputs, outputs = outputs)
model_k.summary()

model_k.compile(loss = "sparse_categorical_crossentropy",
              optimizer = "adam",
              metrics = ["accuracy"])

model_k.fit(X_train_ksm,y_train_ksm,epochs=15,batch_size=15,validation_split=0.1)

y_pred = model_k.predict(X_test_k)

test_pred_class = np.argmax(y_pred, axis=1)
cm = pd.crosstab(y_test_k, test_pred_class, rownames = ['Real'], colnames = ['Pred'])
display(cm)
print(classification_report_imbalanced(y_test_k, test_pred_class))

"""# Conclusion
>Pour ce dataset, le traitement des valeurs manquantes par l’imputation des moyennes et des modalités les plus fréquentes a fourni de meilleurs résultats de prédiction qu’avec la méthode du KnnImputer proposée par Scikit_learn. 
Plusieurs modèle de prédictions ont été testés :
> - régression logistique
> - k-nearest neighbors
> - support vector machines
> - random forest classifier
> - gradient boosting classifier
> - XGboost
>
>Dans l’ensemble on constate que la classe 1, correspondant au fait qu’il pleuve le lendemain, est la classe la plus difficile à prédire.  
>
>Si XGboost obtient un score global de 0.86, il présente des difficultés à prédire la classe 1, le recall pour cette classe est de 0.57, tandis que celui de la classe 0 est de 0.94. Autrement dit, le modèle est très performant pour prédire la classe 0.  
>
>Quant au modèle SVM, bien qu’il ait un score global légèrement inférieur, 0.81, celui-ci est beaucoup plus équilibré dans sa capacité à prédire la classe 1 et 0, puisque ces classes ont respectivement comme recall 0.79 et 0.82. Ce modèle est donc relativement performant, mais il a comme inconvénient d’être très long à entraîner, à l’inverse de XGboost qui est plutôt rapide. 
> 
>Une piste serait de s’appuyer sur les forces et les faiblesses de ces deux modèles. En effet, XGboost est en capacité de fournir une première prédiction rapide et il est très performant pour prédire la classe dominante. En fonction du résultat et de la probabilité que la prédiction appartienne à la classe 1, celle-ci pourrait être confirmée par le modèle SVM.

"""